{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(labels_test, y_pred_round, model_name, fold):\n",
    "    conf_matrix = confusion_matrix(labels_test, y_pred_round)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix ' + model_name + ' fold'+ str(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = None\n",
    "\n",
    "def xg_boost_test(X_train, y_train, X_test, y_test, model_name, fold):\n",
    "    global y_pred\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    xgb_model.load_model(f\"final/best/{model_name}_fold_{fold}.json\")\n",
    "\n",
    "    # Step 6: Make predictions\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Step 7: Evaluate the model using accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')  # Use 'macro' for multi-class precision\n",
    "    recall = recall_score(y_test, y_pred, average='binary')        # Use 'macro' for multi-class recall\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')                # Use 'macro' for multi-class F1 score\n",
    "\n",
    "    # Print the results\n",
    "    print('Model: ' + model_name + ' fold ' + str(fold))\n",
    "    print(f\"Accuracy of XGBoost: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print()\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, model_name, fold)\n",
    "\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "from docx import Document\n",
    "import re\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "window_size = 1\n",
    "\n",
    "def sentence_hierarchy(check_sentence, paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    sentences = [sent.text.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '').strip() for sent in doc.sents]\n",
    "\n",
    "    found = False\n",
    "\n",
    "    text = check_sentence\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if check_sentence.strip() in sentence:\n",
    "            target_index = i\n",
    "            found = True\n",
    "            text = ''\n",
    "\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        print('========================')\n",
    "        print(check_sentence)\n",
    "        print(sentences)\n",
    "    else:\n",
    "        for i_win in range(-window_size,window_size +1):\n",
    "            if i_win != -window_size:\n",
    "                text += \"[SEP]\"\n",
    "\n",
    "            if target_index + i_win < 0 or target_index + i_win >= len(sentences):\n",
    "                text += \"[NULL]\"\n",
    "            else:\n",
    "                text += sentences[target_index + i_win]\n",
    "    return text\n",
    "\n",
    "def extract_sentences_labels(news_file_path, ex_file_path):\n",
    "\n",
    "    with open(news_file_path, 'r', encoding='utf-8') as file:\n",
    "        news_text = file.read().strip()\n",
    "    with open(ex_file_path, 'r', encoding='utf-8') as file:\n",
    "        ex_text = file.read().strip()\n",
    "\n",
    "    lines = news_text.splitlines()\n",
    "    news_text = \"\\n\".join(lines[1:])\n",
    "\n",
    "    doc = nlp(news_text)\n",
    "    # Extract sentences\n",
    "    sentences = [sent.text.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '') for sent in doc.sents]\n",
    "    labels = []\n",
    "    news_paths = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        news_paths.append(news_file_path)\n",
    "\n",
    "        if sentence in ex_text:\n",
    "            labels.append('1')\n",
    "        else:\n",
    "            labels.append('0')\n",
    "\n",
    "    return news_paths, sentences, labels\n",
    "\n",
    "def extract_sentences_paragraphs_labels(news_file_path):\n",
    "\n",
    "    with open(news_file_path, 'r', encoding='utf-8') as file:\n",
    "        news_text = file.read().strip()\n",
    "\n",
    "    lines = news_text.splitlines()\n",
    "    news_text = \"\\n\".join(lines[1:])\n",
    "    \n",
    "    sentences = []\n",
    "    paragraphs = []\n",
    "    hierarchies = []\n",
    "\n",
    "    for par in lines[1:]:\n",
    "        doc = nlp(par)\n",
    "\n",
    "        p_sentences = [sent.text.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '') for sent in doc.sents]\n",
    "        p_paragraphs = [par.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '') for i in p_sentences]\n",
    "        p_hierarchies = [sentence_hierarchy(sent, par) for sent in p_sentences]\n",
    "\n",
    "        sentences += p_sentences\n",
    "        paragraphs += p_paragraphs\n",
    "        hierarchies += p_hierarchies\n",
    "    \n",
    "    labels = []\n",
    "    news_paths = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        news_paths.append(news_file_path)\n",
    "\n",
    "        labels.append('0')\n",
    "\n",
    "    return paragraphs, hierarchies, sentences, labels\n",
    "\n",
    "def extract_dataset_from_files(news_file_path, annotated_file_path):\n",
    "    procedural_sentences = read_docx(annotated_file_path)\n",
    "\n",
    "    with open(news_file_path, 'r', encoding='utf-8') as file:\n",
    "        news_text = file.read().strip()\n",
    "\n",
    "    lines = news_text.splitlines()\n",
    "    news_text = \"\\n\".join(lines[1:])\n",
    "    \n",
    "    sentences = []\n",
    "    paragraphs = []\n",
    "    hierarchies = []\n",
    "    labels = []\n",
    "    paragraph_labels = []\n",
    "\n",
    "    for par in lines[1:]:\n",
    "        doc = nlp(par)\n",
    "\n",
    "        p_sentences = [sent.text.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '') for sent in doc.sents]\n",
    "        p_paragraphs = [par.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '') for i in p_sentences]\n",
    "        p_hierarchies = [sentence_hierarchy(sent, par) for sent in p_sentences]\n",
    "\n",
    "        p_labels = [0] * len(p_sentences)\n",
    "\n",
    "        for sentence in p_sentences:\n",
    "            sentences.append(sentence)\n",
    "            if(check_phrases_in_sentence(sentence, procedural_sentences)):\n",
    "                labels.append(1)\n",
    "                p_labels = [1] * len(p_sentences)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "        # sentences += p_sentences\n",
    "        paragraphs += p_paragraphs\n",
    "        hierarchies += p_hierarchies\n",
    "        paragraph_labels += p_labels\n",
    "\n",
    "    return paragraphs, hierarchies, sentences, labels, paragraph_labels\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return extract_text_inside_brackets('\\n'.join(full_text))\n",
    "\n",
    "def extract_text_inside_brackets(input_string):\n",
    "    # Define the regex pattern\n",
    "    pattern = r'<(.*?)>'\n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, input_string)\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for par in matches:\n",
    "        doc = nlp(par)\n",
    "        \n",
    "        sentences = sentences  + [sent.text.replace('\\r\\n', '').replace('\\n', '').replace('\\r', '').replace('(', '').replace(')', '') for sent in doc.sents]\n",
    "\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def check_phrases_in_sentence(sentence, phrases_array):\n",
    "    # Check if any phrase from the array is present in the sentence\n",
    "    return any(phrase in sentence.replace('(', '').replace(')', '') for phrase in phrases_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def text_embedding(texts, pretrained_model, tokenizer, batch_size = 32, pool_type = 1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Move models to GPU\n",
    "    pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "    n = len(texts)\n",
    "    text = np.zeros((n, pretrained_model.config.hidden_size))\n",
    "    step = 0\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for i in range(0, n, batch_size):\n",
    "        end = min(i + batch_size, n)\n",
    "        batch_texts = texts[i:end]\n",
    "        \n",
    "        if pool_type == 1:\n",
    "            # Tokenize for RoBERTa\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():  # Disable gradient calculation for inference\n",
    "                outputs = pretrained_model(**inputs)\n",
    "            embeddings = outputs.pooler_output.cpu().numpy()\n",
    "\n",
    "        if pool_type == 2:\n",
    "            # Tokenize for SBERT\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():  # Disable gradient calculation for inference\n",
    "                outputs = pretrained_model(**inputs)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = inputs['attention_mask'].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            embeddings = (sum_embeddings / sum_mask).cpu().numpy()  # Move output back to CPU\n",
    "\n",
    "        if pool_type == 3:\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():  # Disable gradient calculation for inference\n",
    "                outputs = pretrained_model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            last_hidden_state = hidden_states[-1]\n",
    "            embeddings = last_hidden_state.mean(dim=1).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "        # Store the results in preallocated arrays\n",
    "        text[i:end] = embeddings\n",
    "\n",
    "        # Estimate and print remaining time\n",
    "        if i % (batch_size * 10) == 0 or end == n:  # Reduce the frequency of print statements\n",
    "            step = end\n",
    "            delta = datetime.now() - start_time\n",
    "            resulting_delta = (delta / step) * (n - step)\n",
    "            hours = resulting_delta.seconds // 3600\n",
    "            minutes = (resulting_delta.seconds // 60) % 60\n",
    "            seconds = resulting_delta.seconds % 60\n",
    "            print(f\"\\rLoading... {step}/{n} | {hours} hr, {minutes} minutes, {seconds} seconds remaining     \", end='')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = 'bbc_news\\\\news_articles'\n",
    "news_category_path = os.path.join(news_path, 'tech')\n",
    "news_file_path = os.path.join(news_category_path, '001.txt')\n",
    "\n",
    "paragraphs, hierarchies, sentences, labels = extract_sentences_paragraphs_labels(news_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PPTI Java\\anaconda3\\envs\\py310\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... 32/32 | 0 hr, 0 minutes, 0 seconds remaining     "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "s_path = 'model_oversampled_xlnet'\n",
    "h_path = 'model_oversampled_roberta_h'\n",
    "\n",
    "tokenizer_s = AutoTokenizer.from_pretrained('jvasdigital/model_oversampled_xlnet')\n",
    "pretrained_model_s = AutoModel.from_pretrained('jvasdigital/model_oversampled_xlnet')\n",
    "\n",
    "\n",
    "tokenizer_h = AutoTokenizer.from_pretrained('jvasdigital/model_oversampled_roberta_h')\n",
    "pretrained_model_h = AutoModel.from_pretrained('jvasdigital/model_oversampled_roberta_h')\n",
    "\n",
    "text_test_s = text_embedding(sentences, pretrained_model_s, tokenizer_s, batch_size = 128, pool_type = 3)\n",
    "text_test_h = text_embedding(hierarchies, pretrained_model_h, tokenizer_h, batch_size = 128, pool_type = 1)\n",
    "\n",
    "combined_data_train = np.concatenate([text_test_h, text_test_s], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted procedural sentences:\n",
      "The Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country's elections as part of a drive to prevent multiple voting.\n",
      "In an effort to live up to its reputation in the 1990s as \"an island of democracy\", the Kyrgyz President, Askar Akaev, pushed through the law requiring the use of ink during the upcoming Parliamentary and Presidential elections.\n",
      "The US government agreed to fund all expenses associated with this decision.\n",
      "However, the presence of ultraviolet light (of the kind used to verify money) causes the ink to glow with a neon yellow light.\n",
      "If the ink shows under the UV light the voter will not be allowed to enter the polling station.\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.load_model(f\"final/best/xlnet + roberta oversampled_fold_1.json\")\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = xgb_model.predict(combined_data_train)\n",
    "\n",
    "extracted = [sentences[i] for i in range(len(sentences)) if y_pred[i] == 1]\n",
    "\n",
    "print('Predicted procedural sentences:')\n",
    "for i in range(len(extracted)):\n",
    "    print(extracted[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
